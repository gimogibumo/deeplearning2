"""
NumPy ê¸°ë°˜ ì‹ ê²½ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
PyTorch/TensorFlow ì—†ì´ ìˆœìˆ˜ NumPyë¡œ êµ¬í˜„

Components:
- Layers: Linear, Embedding, Dropout
- Activations: ReLU, Sigmoid, Softmax, Tanh
- Loss: CrossEntropyLoss
- Optimizers: SGD, Adam
"""

import numpy as np
from typing import List, Tuple, Optional, Dict


# ============================================================================
# ê¸°ë³¸ ë ˆì´ì–´ í´ë˜ìŠ¤
# ============================================================================

class Layer:
    """ëª¨ë“  ë ˆì´ì–´ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.params = {}
        self.grads = {}
        self.cache = {}
        self.training = True
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        raise NotImplementedError
    
    def train(self):
        self.training = True
    
    def eval(self):
        self.training = False


class Linear(Layer):
    """
    Fully Connected Layer (ì„ í˜• ë³€í™˜)
    y = xW + b
    """
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super().__init__()
        
        # Xavier/Glorot ì´ˆê¸°í™”
        scale = np.sqrt(2.0 / (in_features + out_features))
        self.params['W'] = np.random.randn(in_features, out_features) * scale
        
        if bias:
            self.params['b'] = np.zeros(out_features)
        
        self.use_bias = bias
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        Args:
            x: (batch_size, in_features)
        Returns:
            out: (batch_size, out_features)
        """
        self.cache['x'] = x
        out = x @ self.params['W']
        if self.use_bias:
            out = out + self.params['b']
        return out
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        """
        Args:
            grad: (batch_size, out_features) - ìƒìœ„ ë ˆì´ì–´ì—ì„œ ì „íŒŒëœ ê·¸ë˜ë””ì–¸íŠ¸
        Returns:
            dx: (batch_size, in_features) - í•˜ìœ„ ë ˆì´ì–´ë¡œ ì „íŒŒí•  ê·¸ë˜ë””ì–¸íŠ¸
        """
        x = self.cache['x']
        batch_size = x.shape[0]
        
        # íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸
        self.grads['W'] = x.T @ grad / batch_size
        if self.use_bias:
            self.grads['b'] = np.mean(grad, axis=0)
        
        # ì…ë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸
        dx = grad @ self.params['W'].T
        return dx


class Embedding(Layer):
    """
    ì„ë² ë”© ë ˆì´ì–´ (lookup table)
    ë‹¨ì–´ ì¸ë±ìŠ¤ -> ë²¡í„°
    """
    
    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int = 0):
        super().__init__()
        
        # ì„ë² ë”© ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.params['W'] = np.random.randn(num_embeddings, embedding_dim) * 0.01
        self.params['W'][padding_idx] = 0  # íŒ¨ë”©ì€ 0 ë²¡í„°
        
        self.padding_idx = padding_idx
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        Args:
            x: (batch_size, seq_len) - ë‹¨ì–´ ì¸ë±ìŠ¤
        Returns:
            out: (batch_size, seq_len, embedding_dim)
        """
        self.cache['x'] = x
        return self.params['W'][x]
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        """
        Args:
            grad: (batch_size, seq_len, embedding_dim)
        Returns:
            None (ì„ë² ë”©ì€ ì…ë ¥ì´ ì¸ë±ìŠ¤ì´ë¯€ë¡œ)
        """
        x = self.cache['x']
        
        # ì„ë² ë”© ê°€ì¤‘ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸
        self.grads['W'] = np.zeros_like(self.params['W'])
        np.add.at(self.grads['W'], x, grad)
        self.grads['W'] /= x.shape[0]
        
        # íŒ¨ë”© ì¸ë±ìŠ¤ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” 0
        self.grads['W'][self.padding_idx] = 0
        
        return None  # ì¸ë±ìŠ¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì—†ìŒ


class Dropout(Layer):
    """
    ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ (ì •ê·œí™”)
    """
    
    def __init__(self, p: float = 0.5):
        super().__init__()
        self.p = p  # ë“œë¡­ í™•ë¥ 
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        if self.training and self.p > 0:
            self.cache['mask'] = (np.random.rand(*x.shape) > self.p).astype(np.float64)
            return x * self.cache['mask'] / (1 - self.p)
        return x
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        if self.training and self.p > 0:
            return grad * self.cache['mask'] / (1 - self.p)
        return grad


# ============================================================================
# í™œì„±í™” í•¨ìˆ˜
# ============================================================================

class ReLU(Layer):
    """ReLU í™œì„±í™” í•¨ìˆ˜"""
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['x'] = x
        return np.maximum(0, x)
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['x']
        return grad * (x > 0).astype(np.float64)


class Sigmoid(Layer):
    """Sigmoid í™œì„±í™” í•¨ìˆ˜"""
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        # ìˆ˜ì¹˜ ì•ˆì •ì„±
        x = np.clip(x, -500, 500)
        out = 1 / (1 + np.exp(-x))
        self.cache['out'] = out
        return out
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        out = self.cache['out']
        return grad * out * (1 - out)


class Tanh(Layer):
    """Tanh í™œì„±í™” í•¨ìˆ˜"""
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        out = np.tanh(x)
        self.cache['out'] = out
        return out
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        out = self.cache['out']
        return grad * (1 - out ** 2)


class Softmax(Layer):
    """Softmax í™œì„±í™” í•¨ìˆ˜ (ì£¼ë¡œ ì¶œë ¥ì¸µì—ì„œ ì‚¬ìš©)"""
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ê°’ ë¹¼ê¸°
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        out = exp_x / np.sum(exp_x, axis=-1, keepdims=True)
        self.cache['out'] = out
        return out
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        # CrossEntropyLossì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ê°„ë‹¨í•´ì§
        return grad


# ============================================================================
# ì†ì‹¤ í•¨ìˆ˜
# ============================================================================

class CrossEntropyLoss:
    """
    í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜
    Softmax + NLLLoss ê²°í•©
    """
    
    def __init__(self, class_weights: np.ndarray = None):
        self.class_weights = class_weights
    
    def forward(self, logits: np.ndarray, targets: np.ndarray) -> float:
        """
        Args:
            logits: (batch_size, num_classes) - ëª¨ë¸ ì¶œë ¥ (softmax ì „)
            targets: (batch_size,) - ì •ë‹µ ë ˆì´ë¸”
        Returns:
            loss: ìŠ¤ì¹¼ë¼ ì†ì‹¤ê°’
        """
        batch_size = logits.shape[0]
        
        # Softmax
        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
        
        self.cache = {'probs': probs, 'targets': targets, 'batch_size': batch_size}
        
        # ìˆ˜ì¹˜ ì•ˆì •ì„±
        probs_clipped = np.clip(probs, 1e-15, 1 - 1e-15)
        
        # Negative Log Likelihood
        log_probs = -np.log(probs_clipped[np.arange(batch_size), targets])
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
        if self.class_weights is not None:
            weights = self.class_weights[targets]
            loss = np.mean(log_probs * weights)
        else:
            loss = np.mean(log_probs)
        
        return loss
    
    def backward(self) -> np.ndarray:
        """
        Returns:
            grad: (batch_size, num_classes)
        """
        probs = self.cache['probs']
        targets = self.cache['targets']
        batch_size = self.cache['batch_size']
        
        grad = probs.copy()
        grad[np.arange(batch_size), targets] -= 1
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
        if self.class_weights is not None:
            weights = self.class_weights[targets].reshape(-1, 1)
            grad = grad * weights
        
        return grad / batch_size


# ============================================================================
# ì˜µí‹°ë§ˆì´ì €
# ============================================================================

class SGD:
    """í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (ëª¨ë©˜í…€ í¬í•¨)"""
    
    def __init__(self, params: List[Dict], lr: float = 0.01, momentum: float = 0.9):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.velocity = {}
    
    def step(self, layers: List[Layer]):
        for i, layer in enumerate(layers):
            for name, param in layer.params.items():
                if name not in layer.grads:
                    continue
                
                key = f"{i}_{name}"
                if key not in self.velocity:
                    self.velocity[key] = np.zeros_like(param)
                
                # ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸
                self.velocity[key] = self.momentum * self.velocity[key] - self.lr * layer.grads[name]
                layer.params[name] += self.velocity[key]
    
    def zero_grad(self, layers: List[Layer]):
        for layer in layers:
            layer.grads = {}


class Adam:
    """Adam ì˜µí‹°ë§ˆì´ì €"""
    
    def __init__(self, lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = {}  # 1ì°¨ ëª¨ë©˜íŠ¸
        self.v = {}  # 2ì°¨ ëª¨ë©˜íŠ¸
        self.t = 0   # íƒ€ì„ìŠ¤í…
    
    def step(self, layers: List[Layer]):
        self.t += 1
        
        for i, layer in enumerate(layers):
            for name, param in layer.params.items():
                if name not in layer.grads:
                    continue
                
                key = f"{i}_{name}"
                grad = layer.grads[name]
                
                # ëª¨ë©˜íŠ¸ ì´ˆê¸°í™”
                if key not in self.m:
                    self.m[key] = np.zeros_like(param)
                    self.v[key] = np.zeros_like(param)
                
                # ëª¨ë©˜íŠ¸ ì—…ë°ì´íŠ¸
                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad
                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grad ** 2)
                
                # í¸í–¥ ë³´ì •
                m_hat = self.m[key] / (1 - self.beta1 ** self.t)
                v_hat = self.v[key] / (1 - self.beta2 ** self.t)
                
                # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
                layer.params[name] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
    
    def zero_grad(self, layers: List[Layer]):
        for layer in layers:
            layer.grads = {}


# ============================================================================
# ì‹ ê²½ë§ ëª¨ë¸ í´ë˜ìŠ¤
# ============================================================================

class Sequential:
    """ë ˆì´ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•œ ì‹ ê²½ë§"""
    
    def __init__(self, layers: List[Layer]):
        self.layers = layers
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        for layer in self.layers:
            x = layer.forward(x)
        return x
    
    def backward(self, grad: np.ndarray):
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
    
    def train(self):
        for layer in self.layers:
            layer.train()
    
    def eval(self):
        for layer in self.layers:
            layer.eval()
    
    def get_layers(self) -> List[Layer]:
        return self.layers


class MLP:
    """
    Multi-Layer Perceptron (ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )
    í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê¸°ë³¸ ì‹ ê²½ë§
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dims: List[int],
        output_dim: int,
        dropout: float = 0.3
    ):
        layers = []
        
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(Linear(prev_dim, hidden_dim))
            layers.append(ReLU())
            layers.append(Dropout(dropout))
            prev_dim = hidden_dim
        
        layers.append(Linear(prev_dim, output_dim))
        
        self.model = Sequential(layers)
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        return self.model.forward(x)
    
    def backward(self, grad: np.ndarray):
        self.model.backward(grad)
    
    def train(self):
        self.model.train()
    
    def eval(self):
        self.model.eval()
    
    def get_layers(self) -> List[Layer]:
        return self.model.get_layers()


# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def accuracy(predictions: np.ndarray, targets: np.ndarray) -> float:
    """ì •í™•ë„ ê³„ì‚°"""
    return np.mean(predictions == targets)


def save_model(layers: List[Layer], filepath: str):
    """ëª¨ë¸ ì €ì¥"""
    state = {}
    for i, layer in enumerate(layers):
        for name, param in layer.params.items():
            state[f"layer_{i}_{name}"] = param
    np.savez(filepath, **state)


def load_model(layers: List[Layer], filepath: str):
    """ëª¨ë¸ ë¡œë“œ"""
    state = np.load(filepath)
    for i, layer in enumerate(layers):
        for name in layer.params.keys():
            key = f"layer_{i}_{name}"
            if key in state:
                layer.params[name] = state[key]


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª NumPy ì‹ ê²½ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸")
    
    # MLP í…ŒìŠ¤íŠ¸
    np.random.seed(42)
    
    # ë”ë¯¸ ë°ì´í„°
    X = np.random.randn(32, 100)  # 32 ìƒ˜í”Œ, 100 íŠ¹ì„±
    y = np.random.randint(0, 2, 32)  # ì´ì§„ ë¶„ë¥˜
    
    # ëª¨ë¸ ìƒì„±
    model = MLP(input_dim=100, hidden_dims=[64, 32], output_dim=2, dropout=0.3)
    criterion = CrossEntropyLoss()
    optimizer = Adam(lr=0.001)
    
    # í•™ìŠµ ë£¨í”„
    model.train()
    for epoch in range(10):
        # Forward
        logits = model.forward(X)
        loss = criterion.forward(logits, y)
        
        # Backward
        grad = criterion.backward()
        model.backward(grad)
        
        # Update
        optimizer.step(model.get_layers())
        optimizer.zero_grad(model.get_layers())
        
        # Accuracy
        preds = np.argmax(logits, axis=1)
        acc = accuracy(preds, y)
        
        print(f"Epoch {epoch+1}: Loss={loss:.4f}, Accuracy={acc:.4f}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

