"""
Gillam ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ëª¨ë“ˆ
- .cha íŒŒì¼ì—ì„œ ì•„ë™ ë°œí™” í…ìŠ¤íŠ¸ ì¶”ì¶œ
- TF-IDF ë˜ëŠ” Bag-of-Words ë²¡í„°í™”
- Train/Dev/Test ë°ì´í„° ì¤€ë¹„
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from collections import Counter
import re

from utils import extract_utterances


# ============================================================================
# í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ============================================================================

def extract_child_text(cha_file: str, base_dir: str = ".") -> str:
    """
    .cha íŒŒì¼ì—ì„œ ì•„ë™(CHI) ë°œí™”ë§Œ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    
    Args:
        cha_file: íŒŒì¼ ê²½ë¡œ (ì˜ˆ: gillam/SLI/5f/55697il-l.cha)
        base_dir: ê¸°ì¤€ ë””ë ‰í† ë¦¬
    
    Returns:
        ì•„ë™ ë°œí™”ë¥¼ ê²°í•©í•œ í…ìŠ¤íŠ¸
    """
    file_path = Path(base_dir) / cha_file
    
    if not file_path.exists():
        print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {file_path}")
        return ""
    
    try:
        utterances = extract_utterances(str(file_path), ["CHI"])
        texts = [utt.clean_text for utt in utterances if utt.clean_text.strip()]
        return " ".join(texts)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({cha_file}): {e}")
        return ""


def load_split_data(split_dir: str = "split") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Train/Dev/Test split íŒŒì¼ ë¡œë“œ"""
    train_df = pd.read_csv(f"{split_dir}/gillam_train.csv")
    dev_df = pd.read_csv(f"{split_dir}/gillam_dev.csv")
    test_df = pd.read_csv(f"{split_dir}/gillam_test.csv")
    
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   Train: {len(train_df)} (SLI: {sum(train_df['group']=='SLI')}, TD: {sum(train_df['group']=='TD')})")
    print(f"   Dev: {len(dev_df)} (SLI: {sum(dev_df['group']=='SLI')}, TD: {sum(dev_df['group']=='TD')})")
    print(f"   Test: {len(test_df)} (SLI: {sum(test_df['group']=='SLI')}, TD: {sum(test_df['group']=='TD')})")
    
    return train_df, dev_df, test_df


def prepare_texts_and_labels(df: pd.DataFrame, base_dir: str = ".") -> Tuple[List[str], np.ndarray]:
    """
    DataFrameì—ì„œ í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸” ì¶”ì¶œ
    
    Returns:
        texts: ì•„ë™ ë°œí™” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        labels: ë ˆì´ë¸” ë°°ì—´ (SLI=1, TD=0)
    """
    texts = []
    labels = []
    
    for idx, row in df.iterrows():
        text = extract_child_text(row['filename'], base_dir)
        if text:
            texts.append(text)
            labels.append(1 if row['group'] == 'SLI' else 0)
    
    return texts, np.array(labels)


# ============================================================================
# í…ìŠ¤íŠ¸ ë²¡í„°í™” (TF-IDF)
# ============================================================================

class TfidfVectorizer:
    """
    TF-IDF ë²¡í„°í™” (scikit-learn ì—†ì´ ìˆœìˆ˜ êµ¬í˜„)
    """
    
    def __init__(
        self,
        max_features: int = 5000,
        min_df: int = 2,
        max_df: float = 0.95,
        ngram_range: Tuple[int, int] = (1, 2)
    ):
        self.max_features = max_features
        self.min_df = min_df
        self.max_df = max_df
        self.ngram_range = ngram_range
        
        self.vocabulary_ = {}
        self.idf_ = None
        self.feature_names_ = []
    
    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬"""
        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        
        tokens = []
        # Unigrams
        if self.ngram_range[0] <= 1 <= self.ngram_range[1]:
            tokens.extend(words)
        
        # Bigrams
        if self.ngram_range[0] <= 2 <= self.ngram_range[1]:
            for i in range(len(words) - 1):
                tokens.append(f"{words[i]}_{words[i+1]}")
        
        return tokens
    
    def fit(self, texts: List[str]):
        """ì–´íœ˜ ì‚¬ì „ ë° IDF ê³„ì‚°"""
        n_docs = len(texts)
        
        # ë¬¸ì„œ ë¹ˆë„ ê³„ì‚°
        doc_freq = Counter()
        for text in texts:
            unique_tokens = set(self._tokenize(text))
            doc_freq.update(unique_tokens)
        
        # min_df, max_df í•„í„°ë§
        max_doc_count = int(self.max_df * n_docs) if isinstance(self.max_df, float) else self.max_df
        min_doc_count = self.min_df
        
        filtered_tokens = {
            token: freq for token, freq in doc_freq.items()
            if min_doc_count <= freq <= max_doc_count
        }
        
        # ë¹ˆë„ìˆœ ì •ë ¬ í›„ max_features ê°œìˆ˜ë§Œí¼ ì„ íƒ
        sorted_tokens = sorted(filtered_tokens.items(), key=lambda x: -x[1])
        selected_tokens = sorted_tokens[:self.max_features]
        
        # ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
        self.vocabulary_ = {token: idx for idx, (token, _) in enumerate(selected_tokens)}
        self.feature_names_ = [token for token, _ in selected_tokens]
        
        # IDF ê³„ì‚°: log(N / df) + 1
        self.idf_ = np.zeros(len(self.vocabulary_))
        for token, idx in self.vocabulary_.items():
            df = doc_freq[token]
            self.idf_[idx] = np.log(n_docs / df) + 1
        
        print(f"ğŸ“– ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•: {len(self.vocabulary_)} íŠ¹ì„±")
        return self
    
    def transform(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜"""
        n_docs = len(texts)
        n_features = len(self.vocabulary_)
        
        # TF ê³„ì‚°
        tf_matrix = np.zeros((n_docs, n_features))
        
        for i, text in enumerate(texts):
            tokens = self._tokenize(text)
            token_counts = Counter(tokens)
            
            for token, count in token_counts.items():
                if token in self.vocabulary_:
                    idx = self.vocabulary_[token]
                    tf_matrix[i, idx] = count
        
        # TF ì •ê·œí™” (ë¬¸ì„œ ê¸¸ì´ë¡œ ë‚˜ëˆ„ê¸°)
        doc_lengths = tf_matrix.sum(axis=1, keepdims=True)
        doc_lengths[doc_lengths == 0] = 1  # 0 ë‚˜ëˆ” ë°©ì§€
        tf_matrix = tf_matrix / doc_lengths
        
        # TF-IDF ê³„ì‚°
        tfidf_matrix = tf_matrix * self.idf_
        
        # L2 ì •ê·œí™”
        norms = np.linalg.norm(tfidf_matrix, axis=1, keepdims=True)
        norms[norms == 0] = 1
        tfidf_matrix = tfidf_matrix / norms
        
        return tfidf_matrix
    
    def fit_transform(self, texts: List[str]) -> np.ndarray:
        """fitê³¼ transformì„ í•œ ë²ˆì—"""
        self.fit(texts)
        return self.transform(texts)


class BagOfWordsVectorizer:
    """
    Bag-of-Words ë²¡í„°í™” (ë” ê°„ë‹¨í•œ ë°©ë²•)
    """
    
    def __init__(self, max_features: int = 5000, min_df: int = 2):
        self.max_features = max_features
        self.min_df = min_df
        self.vocabulary_ = {}
    
    def _tokenize(self, text: str) -> List[str]:
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        return text.split()
    
    def fit(self, texts: List[str]):
        word_counts = Counter()
        for text in texts:
            word_counts.update(self._tokenize(text))
        
        # min_df í•„í„°ë§ ë° ìƒìœ„ max_features ì„ íƒ
        filtered = {w: c for w, c in word_counts.items() if c >= self.min_df}
        sorted_words = sorted(filtered.items(), key=lambda x: -x[1])[:self.max_features]
        
        self.vocabulary_ = {word: idx for idx, (word, _) in enumerate(sorted_words)}
        print(f"ğŸ“– ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•: {len(self.vocabulary_)} íŠ¹ì„±")
        return self
    
    def transform(self, texts: List[str]) -> np.ndarray:
        n_docs = len(texts)
        n_features = len(self.vocabulary_)
        
        bow_matrix = np.zeros((n_docs, n_features))
        
        for i, text in enumerate(texts):
            for word in self._tokenize(text):
                if word in self.vocabulary_:
                    bow_matrix[i, self.vocabulary_[word]] += 1
        
        # ì •ê·œí™”
        norms = np.linalg.norm(bow_matrix, axis=1, keepdims=True)
        norms[norms == 0] = 1
        bow_matrix = bow_matrix / norms
        
        return bow_matrix
    
    def fit_transform(self, texts: List[str]) -> np.ndarray:
        self.fit(texts)
        return self.transform(texts)


# ============================================================================
# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ============================================================================

class GillamDataset:
    """Gillam ë°ì´í„°ì…‹ ê´€ë¦¬"""
    
    def __init__(
        self,
        vectorizer_type: str = "tfidf",
        max_features: int = 3000,
        base_dir: str = "."
    ):
        self.vectorizer_type = vectorizer_type
        self.max_features = max_features
        self.base_dir = base_dir
        
        if vectorizer_type == "tfidf":
            self.vectorizer = TfidfVectorizer(max_features=max_features)
        else:
            self.vectorizer = BagOfWordsVectorizer(max_features=max_features)
        
        self.X_train = None
        self.y_train = None
        self.X_dev = None
        self.y_dev = None
        self.X_test = None
        self.y_test = None
    
    def load_and_preprocess(self, split_dir: str = "split"):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        train_df, dev_df, test_df = load_split_data(split_dir)
        
        print("\nğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        train_texts, self.y_train = prepare_texts_and_labels(train_df, self.base_dir)
        dev_texts, self.y_dev = prepare_texts_and_labels(dev_df, self.base_dir)
        test_texts, self.y_test = prepare_texts_and_labels(test_df, self.base_dir)
        
        print(f"   Train: {len(train_texts)} ìƒ˜í”Œ")
        print(f"   Dev: {len(dev_texts)} ìƒ˜í”Œ")
        print(f"   Test: {len(test_texts)} ìƒ˜í”Œ")
        
        if len(train_texts) == 0:
            raise ValueError("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. gillam ë°ì´í„°ì…‹ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        print(f"\nğŸ”§ {self.vectorizer_type.upper()} ë²¡í„°í™” ì¤‘...")
        self.X_train = self.vectorizer.fit_transform(train_texts)
        self.X_dev = self.vectorizer.transform(dev_texts)
        self.X_test = self.vectorizer.transform(test_texts)
        
        print(f"   íŠ¹ì„± ì°¨ì›: {self.X_train.shape[1]}")
        
        return self
    
    def get_train_data(self) -> Tuple[np.ndarray, np.ndarray]:
        return self.X_train, self.y_train
    
    def get_dev_data(self) -> Tuple[np.ndarray, np.ndarray]:
        return self.X_dev, self.y_dev
    
    def get_test_data(self) -> Tuple[np.ndarray, np.ndarray]:
        return self.X_test, self.y_test
    
    def get_class_weights(self) -> np.ndarray:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        class_counts = np.bincount(self.y_train)
        total = len(self.y_train)
        weights = total / (len(class_counts) * class_counts)
        return weights.astype(np.float64)


# ============================================================================
# ë°°ì¹˜ ìƒì„±ê¸°
# ============================================================================

def create_batches(
    X: np.ndarray, 
    y: np.ndarray, 
    batch_size: int, 
    shuffle: bool = True
) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
    
    Args:
        X: íŠ¹ì„± í–‰ë ¬ (n_samples, n_features)
        y: ë ˆì´ë¸” ë°°ì—´ (n_samples,)
        batch_size: ë°°ì¹˜ í¬ê¸°
        shuffle: ì…”í”Œ ì—¬ë¶€
    
    Returns:
        ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ [(X_batch, y_batch), ...]
    """
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    
    if shuffle:
        np.random.shuffle(indices)
    
    batches = []
    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]
        batches.append((X[batch_indices], y[batch_indices]))
    
    return batches


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    
    dataset = GillamDataset(vectorizer_type="tfidf", max_features=3000, base_dir=".")
    dataset.load_and_preprocess(split_dir="split")
    
    X_train, y_train = dataset.get_train_data()
    X_dev, y_dev = dataset.get_dev_data()
    X_test, y_test = dataset.get_test_data()
    
    print(f"\nğŸ“Š ë°ì´í„° í˜•íƒœ:")
    print(f"   X_train: {X_train.shape}")
    print(f"   X_dev: {X_dev.shape}")
    print(f"   X_test: {X_test.shape}")
    
    print(f"\nâš–ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {dataset.get_class_weights()}")
    
    # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
    batches = create_batches(X_train, y_train, batch_size=32, shuffle=True)
    print(f"\nğŸ“¦ ë°°ì¹˜ ìˆ˜: {len(batches)}")
    print(f"   ì²« ë²ˆì§¸ ë°°ì¹˜: X={batches[0][0].shape}, y={batches[0][1].shape}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

