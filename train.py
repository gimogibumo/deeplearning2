"""
Gillam SLI vs TD ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- NumPy ê¸°ë°˜ MLP ëª¨ë¸ í•™ìŠµ
- Train/Dev ì„¸íŠ¸ë¡œ í•™ìŠµ ë° ê²€ì¦
- ìµœì  ëª¨ë¸ ì €ì¥
"""

import numpy as np
import argparse
import json
from pathlib import Path
from tqdm import tqdm

from numpy_nn import MLP, CrossEntropyLoss, Adam, SGD, accuracy, save_model, load_model
from data_preprocessing import GillamDataset, create_batches


def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    """
    ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    
    Returns:
        accuracy, precision, recall, f1
    """
    # True Positives, False Positives, False Negatives
    tp = np.sum((y_pred == 1) & (y_true == 1))
    fp = np.sum((y_pred == 1) & (y_true == 0))
    fn = np.sum((y_pred == 0) & (y_true == 1))
    tn = np.sum((y_pred == 0) & (y_true == 0))
    
    acc = (tp + tn) / len(y_true)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': int(tp),
        'fp': int(fp),
        'fn': int(fn),
        'tn': int(tn)
    }


def train_epoch(model, X, y, criterion, optimizer, batch_size: int):
    """í•œ ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    batches = create_batches(X, y, batch_size, shuffle=True)
    
    for X_batch, y_batch in batches:
        # Forward
        logits = model.forward(X_batch)
        loss = criterion.forward(logits, y_batch)
        
        # Backward
        grad = criterion.backward()
        model.backward(grad)
        
        # Update
        optimizer.step(model.get_layers())
        optimizer.zero_grad(model.get_layers())
        
        total_loss += loss * len(y_batch)
        preds = np.argmax(logits, axis=1)
        all_preds.extend(preds)
        all_labels.extend(y_batch)
    
    avg_loss = total_loss / len(y)
    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))
    
    return avg_loss, metrics


def evaluate(model, X, y, criterion, batch_size: int):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    batches = create_batches(X, y, batch_size, shuffle=False)
    
    for X_batch, y_batch in batches:
        logits = model.forward(X_batch)
        loss = criterion.forward(logits, y_batch)
        
        total_loss += loss * len(y_batch)
        preds = np.argmax(logits, axis=1)
        all_preds.extend(preds)
        all_labels.extend(y_batch)
    
    avg_loss = total_loss / len(y)
    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))
    
    return avg_loss, metrics, np.array(all_preds)


def train_model(
    hidden_dims: list = [256, 128],
    epochs: int = 50,
    batch_size: int = 32,
    learning_rate: float = 0.001,
    dropout: float = 0.3,
    vectorizer_type: str = "tfidf",
    max_features: int = 3000,
    optimizer_type: str = "adam",
    base_dir: str = ".",
    save_dir: str = "checkpoints",
    seed: int = 42
):
    """ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜"""
    
    np.random.seed(seed)
    
    # ë°ì´í„° ë¡œë“œ
    print("\n" + "="*60)
    print("ğŸ“‚ ë°ì´í„° ì¤€ë¹„")
    print("="*60)
    
    dataset = GillamDataset(
        vectorizer_type=vectorizer_type,
        max_features=max_features,
        base_dir=base_dir
    )
    dataset.load_and_preprocess(split_dir=f"{base_dir}/split")
    
    X_train, y_train = dataset.get_train_data()
    X_dev, y_dev = dataset.get_dev_data()
    
    input_dim = X_train.shape[1]
    output_dim = 2
    
    # ëª¨ë¸ ìƒì„±
    print("\n" + "="*60)
    print("ğŸ¤– ëª¨ë¸ ì„¤ì •")
    print("="*60)
    print(f"   Input dim: {input_dim}")
    print(f"   Hidden dims: {hidden_dims}")
    print(f"   Output dim: {output_dim}")
    print(f"   Dropout: {dropout}")
    
    model = MLP(
        input_dim=input_dim,
        hidden_dims=hidden_dims,
        output_dim=output_dim,
        dropout=dropout
    )
    
    # ì†ì‹¤ í•¨ìˆ˜ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
    class_weights = dataset.get_class_weights()
    criterion = CrossEntropyLoss(class_weights=class_weights)
    print(f"   Class weights: {class_weights}")
    
    # ì˜µí‹°ë§ˆì´ì €
    if optimizer_type == "adam":
        optimizer = Adam(lr=learning_rate)
    else:
        optimizer = SGD(params=[], lr=learning_rate, momentum=0.9)
    print(f"   Optimizer: {optimizer_type.upper()}")
    print(f"   Learning rate: {learning_rate}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
    save_path = Path(save_dir)
    save_path.mkdir(exist_ok=True)
    
    # í•™ìŠµ
    print("\n" + "="*60)
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘ (epochs={epochs}, batch_size={batch_size})")
    print("="*60)
    
    best_f1 = 0
    best_epoch = 0
    history = {
        'train_loss': [], 'train_acc': [], 'train_f1': [],
        'dev_loss': [], 'dev_acc': [], 'dev_f1': []
    }
    
    best_model_params = None
    
    for epoch in range(epochs):
        # Train
        train_loss, train_metrics = train_epoch(
            model, X_train, y_train, criterion, optimizer, batch_size
        )
        
        # Evaluate
        dev_loss, dev_metrics, _ = evaluate(
            model, X_dev, y_dev, criterion, batch_size
        )
        
        # History ì €ì¥
        history['train_loss'].append(float(train_loss))
        history['train_acc'].append(train_metrics['accuracy'])
        history['train_f1'].append(train_metrics['f1'])
        history['dev_loss'].append(float(dev_loss))
        history['dev_acc'].append(dev_metrics['accuracy'])
        history['dev_f1'].append(dev_metrics['f1'])
        
        # ë¡œê·¸ ì¶œë ¥
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"\nEpoch {epoch+1}/{epochs}")
            print(f"  Train - Loss: {train_loss:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}")
            print(f"  Dev   - Loss: {dev_loss:.4f}, Acc: {dev_metrics['accuracy']:.4f}, F1: {dev_metrics['f1']:.4f}")
        
        # Best ëª¨ë¸ ì €ì¥
        if dev_metrics['f1'] > best_f1:
            best_f1 = dev_metrics['f1']
            best_epoch = epoch + 1
            
            # í˜„ì¬ íŒŒë¼ë¯¸í„° ë³µì‚¬
            best_model_params = {}
            for i, layer in enumerate(model.get_layers()):
                for name, param in layer.params.items():
                    best_model_params[f"layer_{i}_{name}"] = param.copy()
            
            print(f"  ğŸ’¾ Best ëª¨ë¸ ì €ì¥ (F1: {best_f1:.4f})")
    
    # Best ëª¨ë¸ ì €ì¥
    np.savez(save_path / "best_model.npz", **best_model_params)
    
    # ì„¤ì • ì €ì¥
    config = {
        'input_dim': input_dim,
        'hidden_dims': hidden_dims,
        'output_dim': output_dim,
        'dropout': dropout,
        'vectorizer_type': vectorizer_type,
        'max_features': max_features,
        'best_epoch': best_epoch,
        'best_dev_f1': best_f1
    }
    
    with open(save_path / "config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    # íˆìŠ¤í† ë¦¬ ì €ì¥
    with open(save_path / "history.json", 'w') as f:
        json.dump(history, f, indent=2)
    
    print("\n" + "="*60)
    print(f"ğŸ† í•™ìŠµ ì™„ë£Œ!")
    print("="*60)
    print(f"   Best Epoch: {best_epoch}")
    print(f"   Best Dev F1: {best_f1:.4f}")
    print(f"   ëª¨ë¸ ì €ì¥: {save_path}/best_model.npz")
    
    return {
        'best_epoch': best_epoch,
        'best_dev_f1': best_f1,
        'history': history,
        'config': config
    }


def main():
    parser = argparse.ArgumentParser(description="Gillam SLI/TD ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")
    
    parser.add_argument("--hidden_dims", type=str, default="256,128",
                        help="ì€ë‹‰ì¸µ ì°¨ì› (ì‰¼í‘œë¡œ êµ¬ë¶„)")
    parser.add_argument("--epochs", type=int, default=50,
                        help="í•™ìŠµ ì—í­ ìˆ˜")
    parser.add_argument("--batch_size", type=int, default=32,
                        help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=0.001,
                        help="í•™ìŠµë¥ ")
    parser.add_argument("--dropout", type=float, default=0.3,
                        help="ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨")
    parser.add_argument("--vectorizer", type=str, default="tfidf",
                        choices=["tfidf", "bow"],
                        help="ë²¡í„°í™” ë°©ë²•")
    parser.add_argument("--max_features", type=int, default=3000,
                        help="ìµœëŒ€ íŠ¹ì„± ìˆ˜")
    parser.add_argument("--optimizer", type=str, default="adam",
                        choices=["adam", "sgd"],
                        help="ì˜µí‹°ë§ˆì´ì €")
    parser.add_argument("--base_dir", type=str, default=".",
                        help="ê¸°ë³¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--save_dir", type=str, default="checkpoints",
                        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--seed", type=int, default=42,
                        help="ëœë¤ ì‹œë“œ")
    
    args = parser.parse_args()
    
    # hidden_dims íŒŒì‹±
    hidden_dims = [int(x) for x in args.hidden_dims.split(",")]
    
    train_model(
        hidden_dims=hidden_dims,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        dropout=args.dropout,
        vectorizer_type=args.vectorizer,
        max_features=args.max_features,
        optimizer_type=args.optimizer,
        base_dir=args.base_dir,
        save_dir=args.save_dir,
        seed=args.seed
    )


if __name__ == "__main__":
    main()
