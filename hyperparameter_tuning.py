"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
Grid Searchë¥¼ í†µí•œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
"""

import numpy as np
import json
import itertools
from datetime import datetime
from pathlib import Path

from numpy_nn import MLP, CrossEntropyLoss, Adam, SGD
from data_preprocessing import GillamDataset, create_batches
from train import train_epoch, evaluate, compute_metrics


def grid_search(
    base_dir: str = ".",
    save_dir: str = "tuning_results",
    seed: int = 42
):
    """
    Grid Searchë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    """
    np.random.seed(seed)
    
    save_path = Path(save_dir)
    save_path.mkdir(exist_ok=True)
    
    # íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„
    param_grid = {
        'hidden_dims': [[256, 128], [512, 256], [256, 128, 64], [128, 64]],
        'learning_rate': [0.0005, 0.001, 0.002],
        'dropout': [0.2, 0.3, 0.5],
        'batch_size': [16, 32],
        'max_features': [2000, 3000, 4000]
    }
    
    # ëª¨ë“  ì¡°í•© ìƒì„±
    keys = param_grid.keys()
    combinations = list(itertools.product(*param_grid.values()))
    
    print("="*60)
    print("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    print("="*60)
    print(f"   ì´ ì¡°í•© ìˆ˜: {len(combinations)}")
    print(f"   íƒìƒ‰ ê³µê°„:")
    for key, values in param_grid.items():
        print(f"      {key}: {values}")
    
    results = []
    best_f1 = 0
    best_config = None
    best_model_params = None
    
    # ë°ì´í„°ëŠ” í•œ ë²ˆë§Œ ë¡œë“œ (max_featuresëŠ” ë‚˜ì¤‘ì— ì¡°ì •)
    print("\nğŸ“‚ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    for i, values in enumerate(combinations):
        config = dict(zip(keys, values))
        
        print(f"\n{'='*60}")
        print(f"ğŸ§ª ì‹¤í—˜ {i+1}/{len(combinations)}")
        print(f"   Config: {config}")
        print(f"{'='*60}")
        
        try:
            # ë°ì´í„° ë¡œë“œ (max_features ì ìš©)
            dataset = GillamDataset(
                vectorizer_type="tfidf",
                max_features=config['max_features'],
                base_dir=base_dir
            )
            dataset.load_and_preprocess(split_dir=f"{base_dir}/split")
            
            X_train, y_train = dataset.get_train_data()
            X_dev, y_dev = dataset.get_dev_data()
            
            input_dim = X_train.shape[1]
            
            # ëª¨ë¸ ìƒì„±
            model = MLP(
                input_dim=input_dim,
                hidden_dims=config['hidden_dims'],
                output_dim=2,
                dropout=config['dropout']
            )
            
            # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
            class_weights = dataset.get_class_weights()
            criterion = CrossEntropyLoss(class_weights=class_weights)
            optimizer = Adam(lr=config['learning_rate'])
            
            # í•™ìŠµ (ê³ ì • ì—í­)
            epochs = 30
            exp_best_f1 = 0
            exp_best_epoch = 0
            exp_best_params = None
            
            for epoch in range(epochs):
                train_loss, train_metrics = train_epoch(
                    model, X_train, y_train, criterion, optimizer, config['batch_size']
                )
                dev_loss, dev_metrics, _ = evaluate(
                    model, X_dev, y_dev, criterion, config['batch_size']
                )
                
                if dev_metrics['f1'] > exp_best_f1:
                    exp_best_f1 = dev_metrics['f1']
                    exp_best_epoch = epoch + 1
                    
                    # íŒŒë¼ë¯¸í„° ë³µì‚¬
                    exp_best_params = {}
                    for j, layer in enumerate(model.get_layers()):
                        for name, param in layer.params.items():
                            exp_best_params[f"layer_{j}_{name}"] = param.copy()
            
            print(f"   Best Epoch: {exp_best_epoch}, Best Dev F1: {exp_best_f1:.4f}")
            
            # ê²°ê³¼ ê¸°ë¡
            experiment = {
                'experiment_id': i + 1,
                'config': config,
                'best_epoch': exp_best_epoch,
                'best_dev_f1': exp_best_f1
            }
            results.append(experiment)
            
            # ì „ì²´ Best ì—…ë°ì´íŠ¸
            if exp_best_f1 > best_f1:
                best_f1 = exp_best_f1
                best_config = config.copy()
                best_config['best_epoch'] = exp_best_epoch
                best_model_params = exp_best_params
                print(f"   ğŸ† ìƒˆë¡œìš´ Best! F1: {best_f1:.4f}")
        
        except Exception as e:
            print(f"   âŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
            continue
    
    # Best ëª¨ë¸ ì €ì¥
    if best_model_params is not None:
        np.savez(save_path / "best_tuned_model.npz", **best_model_params)
        
        # Best ì„¤ì • ì €ì¥
        best_config_full = {
            'input_dim': best_config['max_features'],  # ê·¼ì‚¬ê°’
            'hidden_dims': best_config['hidden_dims'],
            'output_dim': 2,
            'dropout': best_config['dropout'],
            'learning_rate': best_config['learning_rate'],
            'batch_size': best_config['batch_size'],
            'max_features': best_config['max_features'],
            'best_epoch': best_config['best_epoch'],
            'best_dev_f1': best_f1
        }
        
        with open(save_path / "best_config.json", 'w') as f:
            json.dump(best_config_full, f, indent=2)
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    tuning_results = {
        'param_grid': {k: [str(v) if isinstance(v, list) else v for v in vals] 
                       for k, vals in param_grid.items()},
        'total_experiments': len(combinations),
        'successful_experiments': len(results),
        'best_config': best_config,
        'best_dev_f1': best_f1,
        'all_results': sorted(results, key=lambda x: x['best_dev_f1'], reverse=True),
        'timestamp': datetime.now().isoformat()
    }
    
    with open(save_path / "tuning_results.json", 'w') as f:
        json.dump(tuning_results, f, indent=2, default=str)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    print("="*60)
    print(f"   ì„±ê³µí•œ ì‹¤í—˜: {len(results)}/{len(combinations)}")
    print(f"   Best Dev F1: {best_f1:.4f}")
    print(f"   Best Config: {best_config}")
    
    # Top 5 ê²°ê³¼
    print(f"\nğŸ“‹ Top 5 ê²°ê³¼:")
    for rank, exp in enumerate(sorted(results, key=lambda x: x['best_dev_f1'], reverse=True)[:5], 1):
        print(f"   {rank}. F1: {exp['best_dev_f1']:.4f}")
        print(f"      {exp['config']}")
    
    print(f"\n   ê²°ê³¼ ì €ì¥: {save_path}")
    
    return tuning_results


def quick_search(
    base_dir: str = ".",
    save_dir: str = "tuning_results",
    seed: int = 42
):
    """
    ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ (ì¶•ì†Œëœ ê³µê°„)
    """
    np.random.seed(seed)
    
    save_path = Path(save_dir)
    save_path.mkdir(exist_ok=True)
    
    # ì¶•ì†Œëœ íƒìƒ‰ ê³µê°„
    param_grid = {
        'hidden_dims': [[256, 128], [512, 256]],
        'learning_rate': [0.001, 0.002],
        'dropout': [0.3, 0.5],
        'batch_size': [32],
        'max_features': [3000]
    }
    
    keys = param_grid.keys()
    combinations = list(itertools.product(*param_grid.values()))
    
    print("="*60)
    print("âš¡ ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰")
    print("="*60)
    print(f"   ì´ ì¡°í•© ìˆ˜: {len(combinations)}")
    
    results = []
    best_f1 = 0
    best_config = None
    best_model_params = None
    
    for i, values in enumerate(combinations):
        config = dict(zip(keys, values))
        
        print(f"\nì‹¤í—˜ {i+1}/{len(combinations)}: {config}")
        
        try:
            dataset = GillamDataset(
                vectorizer_type="tfidf",
                max_features=config['max_features'],
                base_dir=base_dir
            )
            dataset.load_and_preprocess(split_dir=f"{base_dir}/split")
            
            X_train, y_train = dataset.get_train_data()
            X_dev, y_dev = dataset.get_dev_data()
            
            model = MLP(
                input_dim=X_train.shape[1],
                hidden_dims=config['hidden_dims'],
                output_dim=2,
                dropout=config['dropout']
            )
            
            criterion = CrossEntropyLoss(class_weights=dataset.get_class_weights())
            optimizer = Adam(lr=config['learning_rate'])
            
            exp_best_f1 = 0
            exp_best_params = None
            
            for epoch in range(30):
                train_epoch(model, X_train, y_train, criterion, optimizer, config['batch_size'])
                _, dev_metrics, _ = evaluate(model, X_dev, y_dev, criterion, config['batch_size'])
                
                if dev_metrics['f1'] > exp_best_f1:
                    exp_best_f1 = dev_metrics['f1']
                    exp_best_params = {}
                    for j, layer in enumerate(model.get_layers()):
                        for name, param in layer.params.items():
                            exp_best_params[f"layer_{j}_{name}"] = param.copy()
            
            print(f"   Dev F1: {exp_best_f1:.4f}")
            
            results.append({
                'config': config,
                'best_dev_f1': exp_best_f1
            })
            
            if exp_best_f1 > best_f1:
                best_f1 = exp_best_f1
                best_config = config
                best_model_params = exp_best_params
                print(f"   ğŸ† New Best!")
        
        except Exception as e:
            print(f"   âŒ ì‹¤íŒ¨: {e}")
    
    # ì €ì¥
    if best_model_params:
        np.savez(save_path / "best_tuned_model.npz", **best_model_params)
        
        with open(save_path / "best_config.json", 'w') as f:
            json.dump({
                'hidden_dims': best_config['hidden_dims'],
                'dropout': best_config['dropout'],
                'learning_rate': best_config['learning_rate'],
                'batch_size': best_config['batch_size'],
                'max_features': best_config['max_features'],
                'best_dev_f1': best_f1
            }, f, indent=2)
    
    print(f"\nğŸ† Best F1: {best_f1:.4f}")
    print(f"   Config: {best_config}")
    
    return best_config, best_f1


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    parser.add_argument("--mode", type=str, default="quick",
                        choices=["quick", "full"],
                        help="íƒìƒ‰ ëª¨ë“œ (quick: ë¹ ë¥¸ íƒìƒ‰, full: ì „ì²´ íƒìƒ‰)")
    parser.add_argument("--base_dir", type=str, default=".")
    parser.add_argument("--save_dir", type=str, default="tuning_results")
    parser.add_argument("--seed", type=int, default=42)
    
    args = parser.parse_args()
    
    if args.mode == "quick":
        quick_search(args.base_dir, args.save_dir, args.seed)
    else:
        grid_search(args.base_dir, args.save_dir, args.seed)
